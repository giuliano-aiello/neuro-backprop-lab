\begin{section}{Comparisons}
    \par The four distinct techniques proposed for~\glsxtrshort{rprop} are compared through a structured experimental workflow.
    \par Each technique is measured in terms of loss and accuracy. The loss function adopted is the cross-entropy loss:
    \[
        \sum_{n=1}^{N} - \sum_{k=1}^{c} t_{k}^{n} \ln y_{k}^{n}
    \]
    \par A total of $60,000$ elements was instantiated for the dataset, with $50,000$ allocated for the training set and $10,000$ for the evaluation set, using batch sizes of $5,000$ and $250$, respectively.\\
    The learning rate is set as $0.001$, the procedure was performed over $50$ epochs. A coherent configuration file varying on a specific~\glsxtrshort{rprop} version is:
    \begin{verbatim}
        "criterion": "cross_entropy",
        "optimizer": <rproptechnique>,
        "learning_rate": 0.001,
        "epochs": 50,
        "train_set_size": 50000,
        "train_batch_size": 5000,
        "eval_set_size": 10000,
        "eval_batch_size": 250
    \end{verbatim}
    \begin{figure}[h!]
        \centering
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includesvg[inkscapelatex=true,width=\textwidth]{training_loss.svg}
            \label{fig:training_loss}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includesvg[inkscapelatex=true,width=\textwidth]{evaluation_loss.svg}
            \label{fig:evaluation_loss}
        \end{subfigure}
    \end{figure}
    \par It can be instantly observed that the three different customized versions of~\glsxtrshort{rprop} exhibit the same loss trend during the training phase: during the initial epochs, a steep decrease in loss is followed by a gradual decrease that persists until the final epoch.\\
    \glsxtrshort{rprop}\textsuperscript{+} by PyTorch case stands out: up to approximately the 26th epoch, the loss follows the pattern of the other~\glsxtrshort{rprop} versions, while also achieving the lowest loss. But onwards, it deviates from the general pattern with a significant loss increase. In this case, an early stopping criterion could be useful, as a considerably lower number of epochs is required for this~\glsxtrshort{rprop} implementation compared to the others.
    Without any surprises, it is concluded that the PyTorch version of~\glsxtrshort{rprop}\textsuperscript{+} slightly distinguishes itself from the other versions.\\
    \par Almost the same applies during the evaluation phase. However, a slight but greater discrepancy can be observed between the three customized versions from the point at which the gradual decrease begins. Whereas the~\glsxtrshort{rprop}\textsuperscript{+} PyTorch version shows a greater error increase from the second half of the epochs onwards, compared to the training phase.
\end{section}